{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff9d9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from torch_geometric.utils import dense_to_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c03d6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = torch.load('train.pt')\n",
    "val_data = torch.load('val.pt')\n",
    "test_data = torch.load('test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f57735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def prepare_dataset(data):\n",
    "    eeg_list = []\n",
    "    stim_list = []\n",
    "    for eeg, stim in data:\n",
    "        eeg_list.append(eeg.float())           # (320, 64)\n",
    "        stim_list.append(stim.float())         # (320,)\n",
    "    eeg_tensor = torch.stack(eeg_list)         # (N, 320, 64)\n",
    "    stim_tensor = torch.stack(stim_list)       # (N, 320)\n",
    "    return eeg_tensor, stim_tensor\n",
    "\n",
    "X_train, y_train = prepare_dataset(train_data)\n",
    "X_val, y_val = prepare_dataset(val_data)\n",
    "X_test, y_test = prepare_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de75033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    'subset_ratio': 0.01,  # Use 1% of the data\n",
    "    'num_epochs': 5,       # Low epoch for initial testing\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.001,\n",
    "    'tfcn_hidden_channels': 32,  # TGCN hidden channels\n",
    "    'cnn_filters': 64,          # CNN filters\n",
    "    'lstm_hidden_size': 128,    # LSTM hidden size\n",
    "    'lstm_num_layers': 2,       # LSTM layers\n",
    "    'loop_iterations': 4,       # Number of TGCN+CNN+LSTM loops\n",
    "    'cnn_combine_filters': 32,  # CNN filters in combine block\n",
    "    'mlp_hidden_sizes': [128, 64],  # MLP hidden layers\n",
    "    'dropout': 0.3,\n",
    "    'num_nodes': 64,  # Number of EEG channels\n",
    "    'time_steps': 320  # Time steps in EEG data\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d1a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Graph Convolutional Layer\n",
    "class TGCN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TGCN, self).__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # x: (batch, time, nodes, features)\n",
    "        batch_size, time_steps, nodes, features = x.size()\n",
    "        x_out = []\n",
    "        for t in range(time_steps):\n",
    "            x_t = x[:, t, :, :]  # (batch, nodes, features)\n",
    "            x_t = self.conv(x_t, edge_index)\n",
    "            x_out.append(x_t.unsqueeze(1))  # (batch, 1, nodes, out_channels)\n",
    "        return torch.cat(x_out, dim=1)  # (batch, time, nodes, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f1ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Block (TGCN + CNN + LSTM)\n",
    "class SingleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, cnn_filters, lstm_hidden_size, lstm_num_layers, num_nodes, time_steps, dropout):\n",
    "        super(SingleBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_channels = in_channels\n",
    "        for _ in range(HYPERPARAMS['loop_iterations']):\n",
    "            # TGCN\n",
    "            tgcn = TGCN(current_channels, hidden_channels)\n",
    "            # CNN\n",
    "            cnn = nn.Sequential(\n",
    "                nn.Conv2d(hidden_channels, cnn_filters, kernel_size=(3, 3), padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            # LSTM\n",
    "            lstm = nn.LSTM(cnn_filters * num_nodes, lstm_hidden_size, lstm_num_layers, batch_first=True, dropout=dropout if lstm_num_layers > 1 else 0)\n",
    "            self.layers.append(nn.ModuleDict({\n",
    "                'tgcn': tgcn,\n",
    "                'cnn': cnn,\n",
    "                'lstm': lstm\n",
    "            }))\n",
    "            current_channels = lstm_hidden_size\n",
    "        # Output layer\n",
    "        self.out = nn.Linear(lstm_hidden_size, time_steps)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # x: (batch, time, nodes, features)\n",
    "        for layer in self.layers:\n",
    "            # TGCN\n",
    "            x = layer['tgcn'](x, edge_index)  # (batch, time, nodes, hidden_channels)\n",
    "            # CNN\n",
    "            x = x.permute(0, 3, 1, 2)  # (batch, channels, time, nodes)\n",
    "            x = layer['cnn'](x)  # (batch, cnn_filters, time, nodes)\n",
    "            x = x.permute(0, 2, 1, 3)  # (batch, time, channels, nodes)\n",
    "            batch, time, channels, nodes = x.size()\n",
    "            x = x.reshape(batch, time, channels * nodes)  # (batch, time, channels*nodes)\n",
    "            # LSTM\n",
    "            x, _ = layer['lstm'](x)  # (batch, time, lstm_hidden_size)\n",
    "        # Output\n",
    "        x = self.out(x)  # (batch, time, time_steps)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c0d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Model\n",
    "class EEGEnvModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGEnvModel, self).__init__()\n",
    "        # Create adjacency matrix (simple identity for demo, replace with real graph)\n",
    "        adj = torch.eye(HYPERPARAMS['num_nodes']).to(device)\n",
    "        edge_index, _ = dense_to_sparse(adj)\n",
    "        self.register_buffer('edge_index', edge_index)\n",
    "        \n",
    "        # Three parallel blocks\n",
    "        self.block_pearson = SingleBlock(\n",
    "            in_channels=1,  # Assuming single feature per node\n",
    "            hidden_channels=HYPERPARAMS['tfcn_hidden_channels'],\n",
    "            cnn_filters=HYPERPARAMS['cnn_filters'],\n",
    "            lstm_hidden_size=HYPERPARAMS['lstm_hidden_size'],\n",
    "            lstm_num_layers=HYPERPARAMS['lstm_num_layers'],\n",
    "            num_nodes=HYPERPARAMS['num_nodes'],\n",
    "            time_steps=HYPERPARAMS['time_steps'],\n",
    "            dropout=HYPERPARAMS['dropout']\n",
    "        )\n",
    "        self.block_cosine = SingleBlock(\n",
    "            in_channels=1,\n",
    "            hidden_channels=HYPERPARAMS['tfcn_hidden_channels'],\n",
    "            cnn_filters=HYPERPARAMS['cnn_filters'],\n",
    "            lstm_hidden_size=HYPERPARAMS['lstm_hidden_size'],\n",
    "            lstm_num_layers=HYPERPARAMS['lstm_num_layers'],\n",
    "            num_nodes=HYPERPARAMS['num_nodes'],\n",
    "            time_steps=HYPERPARAMS['time_steps'],\n",
    "            dropout=HYPERPARAMS['dropout']\n",
    "        )\n",
    "        self.block_mse = SingleBlock(\n",
    "            in_channels=1,\n",
    "            hidden_channels=HYPERPARAMS['tfcn_hidden_channels'],\n",
    "            cnn_filters=HYPERPARAMS['cnn_filters'],\n",
    "            lstm_hidden_size=HYPERPARAMS['lstm_hidden_size'],\n",
    "            lstm_num_layers=HYPERPARAMS['lstm_num_layers'],\n",
    "            num_nodes=HYPERPARAMS['num_nodes'],\n",
    "            time_steps=HYPERPARAMS['time_steps'],\n",
    "            dropout=HYPERPARAMS['dropout']\n",
    "        )\n",
    "        # Combine block\n",
    "        self.combine_cnn = nn.Sequential(\n",
    "            nn.Conv1d(3 * HYPERPARAMS['time_steps'], HYPERPARAMS['cnn_combine_filters'], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(HYPERPARAMS['dropout'])\n",
    "        )\n",
    "        mlp_layers = []\n",
    "        in_features = HYPERPARAMS['cnn_combine_filters'] * HYPERPARAMS['time_steps']\n",
    "        for hidden_size in HYPERPARAMS['mlp_hidden_sizes']:\n",
    "            mlp_layers.extend([\n",
    "                nn.Linear(in_features, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(HYPERPARAMS['dropout'])\n",
    "            ])\n",
    "            in_features = hidden_size\n",
    "        mlp_layers.append(nn.Linear(in_features, HYPERPARAMS['time_steps']))\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, nodes)\n",
    "        x = x.unsqueeze(-1)  # (batch, time, nodes, 1)\n",
    "        # Parallel blocks\n",
    "        out_pearson = self.block_pearson(x, self.edge_index)  # (batch, time, time_steps)\n",
    "        out_cosine = self.block_cosine(x, self.edge_index)\n",
    "        out_mse = self.block_mse(x, self.edge_index)\n",
    "        # Combine\n",
    "        combined = torch.cat([out_pearson, out_cosine, out_mse], dim=2)  # (batch, time, 3*time_steps)\n",
    "        combined = combined.permute(0, 2, 1)  # (batch, 3*time_steps, time)\n",
    "        combined = self.combine_cnn(combined)  # (batch, cnn_combine_filters, time)\n",
    "        combined = combined.permute(0, 2, 1).reshape(combined.size(0), -1)  # (batch, time*cnn_combine_filters)\n",
    "        out = self.mlp(combined)  # (batch, time_steps)\n",
    "        return out_pearson, out_cosine, out_mse, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d502a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "def pearson_loss(pred, target):\n",
    "    pred = pred - pred.mean(dim=1, keepdim=True)\n",
    "    target = target - target.mean(dim=1, keepdim=True)\n",
    "    num = (pred * target).sum(dim=1)\n",
    "    denom = torch.sqrt((pred ** 2).sum(dim=1) * (target ** 2).sum(dim=1))\n",
    "    return - (num / (denom + 1e-8)).mean()\n",
    "\n",
    "cosine_loss = nn.CosineEmbeddingLoss()\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40978bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, X_train, y_train, X_val, y_val):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=HYPERPARAMS['learning_rate'])\n",
    "    train_losses = {'pearson': [], 'cosine': [], 'mse': [], 'combined': []}\n",
    "    val_metrics = {'pearson': {'mse': [], 'pearson': [], 'cosine': []},\n",
    "                   'cosine': {'mse': [], 'pearson': [], 'cosine': []},\n",
    "                   'mse': {'mse': [], 'pearson': [], 'cosine': []},\n",
    "                   'combined': {'mse': [], 'pearson': [], 'cosine': []}}\n",
    "    \n",
    "    # Subsample 1% of data\n",
    "    num_samples = int(len(X_train) * HYPERPARAMS['subset_ratio'])\n",
    "    indices = torch.randperm(len(X_train))[:num_samples]\n",
    "    X_train_subset = X_train[indices].to(device)\n",
    "    y_train_subset = y_train[indices].to(device)\n",
    "    \n",
    "    for epoch in range(HYPERPARAMS['num_epochs']):\n",
    "        model.train()\n",
    "        epoch_losses = {'pearson': 0, 'cosine': 0, 'mse': 0, 'combined': 0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Training loop with tqdm\n",
    "        for i in tqdm(range(0, len(X_train_subset), HYPERPARAMS['batch_size']), desc=f'Epoch {epoch+1}'):\n",
    "            batch_X = X_train_subset[i:i+HYPERPARAMS['batch_size']]\n",
    "            batch_y = y_train_subset[i:i+HYPERPARAMS['batch_size']]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out_pearson, out_cosine, out_mse, out_combined = model(batch_X)\n",
    "            \n",
    "            # Losses\n",
    "            loss_pearson = pearson_loss(out_pearson, batch_y)\n",
    "            loss_cosine = cosine_loss(out_cosine.reshape(-1), batch_y.reshape(-1), torch.ones(batch_y.size(0)).to(device))\n",
    "            loss_mse = mse_loss(out_mse, batch_y)\n",
    "            loss_combined = mse_loss(out_combined, batch_y)  # Combined block uses MSE\n",
    "            \n",
    "            total_loss = loss_pearson + loss_cosine + loss_mse + loss_combined\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses['pearson'] += loss_pearson.item()\n",
    "            epoch_losses['cosine'] += loss_cosine.item()\n",
    "            epoch_losses['mse'] += loss_mse.item()\n",
    "            epoch_losses['combined'] += loss_combined.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average losses\n",
    "        for key in epoch_losses:\n",
    "            train_losses[key].append(epoch_losses[key] / num_batches)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out_pearson, out_cosine, out_mse, out_combined = model(X_val.to(device))\n",
    "            outputs = {\n",
    "                'pearson': out_pearson,\n",
    "                'cosine': out_cosine,\n",
    "                'mse': out_mse,\n",
    "                'combined': out_combined\n",
    "            }\n",
    "            y_val_device = y_val.to(device)\n",
    "            \n",
    "            for block in outputs:\n",
    "                pred = outputs[block].cpu().numpy()\n",
    "                true = y_val.cpu().numpy()\n",
    "                \n",
    "                # MSE\n",
    "                mse = np.mean((pred - true) ** 2)\n",
    "                val_metrics[block]['mse'].append(mse)\n",
    "                \n",
    "                # Pearson\n",
    "                pearsons = [pearsonr(pred[i], true[i])[0] for i in range(len(pred))]\n",
    "                val_metrics[block]['pearson'].append(np.mean(pearsons))\n",
    "                \n",
    "                # Cosine\n",
    "                cos_sim = np.mean([np.dot(pred[i], true[i]) / (np.linalg.norm(pred[i]) * np.linalg.norm(true[i]) + 1e-8) for i in range(len(pred))])\n",
    "                val_metrics[block]['cosine'].append(cos_sim)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"\\nEpoch {epoch+1} Validation Metrics:\")\n",
    "        for block in val_metrics:\n",
    "            print(f\"{block.capitalize()} Block - MSE: {val_metrics[block]['mse'][-1]:.4f}, \"\n",
    "                  f\"Pearson: {val_metrics[block]['pearson'][-1]:.4f}, \"\n",
    "                  f\"Cosine: {val_metrics[block]['cosine'][-1]:.4f}\")\n",
    "    \n",
    "    return train_losses, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf5c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Function\n",
    "def plot_results(train_losses, val_metrics, X_test, y_test, model):\n",
    "    # Loss Curves\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    metrics = ['pearson', 'cosine', 'mse', 'combined']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        axes[i].plot(train_losses[metric], label='Train Loss')\n",
    "        axes[i].plot(val_metrics[metric]['mse'], label='Val MSE')\n",
    "        axes[i].set_title(f'{metric.capitalize()} Block Loss')\n",
    "        axes[i].set_xlabel('Epoch')\n",
    "        axes[i].set_ylabel('Loss')\n",
    "        axes[i].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test Regression Plots\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, _, _, out_combined = model(X_test[:5].to(device))\n",
    "        pred = out_combined.cpu().numpy()\n",
    "        true = y_test[:5].numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 1, figsize=(10, 15))\n",
    "    for i in range(5):\n",
    "        axes[i].plot(true[i], label='Original')\n",
    "        axes[i].plot(pred[i], label='Predicted')\n",
    "        axes[i].set_title(f'Test Sample {i+1}')\n",
    "        axes[i].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "925a9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                                   | 0/58 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize and train model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m EEGEnvModel()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m train_losses, val_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     24\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m y_train_subset[i:i\u001b[38;5;241m+\u001b[39mHYPERPARAMS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m out_pearson, out_cosine, out_mse, out_combined \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Losses\u001b[39;00m\n\u001b[1;32m     30\u001b[0m loss_pearson \u001b[38;5;241m=\u001b[39m pearson_loss(out_pearson, batch_y)\n",
      "File \u001b[0;32m/DATAHDD/chailex/anaconda3/envs/mtech_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DATAHDD/chailex/anaconda3/envs/mtech_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m, in \u001b[0;36mEEGEnvModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch, time, nodes, 1)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Parallel blocks\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m out_pearson \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_pearson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, time, time_steps)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m out_cosine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_cosine(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m     65\u001b[0m out_mse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_mse(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_index)\n",
      "File \u001b[0;32m/DATAHDD/chailex/anaconda3/envs/mtech_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DATAHDD/chailex/anaconda3/envs/mtech_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mSingleBlock.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# x: (batch, time, nodes, features)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# TGCN\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtgcn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, time, nodes, hidden_channels)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# CNN\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch, channels, time, nodes)\u001b[39;00m\n",
      "File \u001b[0;32m/DATAHDD/chailex/anaconda3/envs/mtech_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DATAHDD/chailex/anaconda3/envs/mtech_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mTGCN.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# x: (batch, time, nodes, features)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     batch_size, time_steps, nodes, features \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     10\u001b[0m     x_out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(time_steps):\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize and train model\n",
    "model = EEGEnvModel().to(device)\n",
    "train_losses, val_metrics = train_model(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f402e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plot_results(train_losses, val_metrics, X_test, y_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mtech_env)",
   "language": "python",
   "name": "mtech_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
